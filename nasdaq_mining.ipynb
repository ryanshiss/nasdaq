{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1LQz_Dq9qkLIh4qkqx7pnrxdG1K3tWYh6","authorship_tag":"ABX9TyOX5Fefic2Wz+e1/+wGCMIu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Mount my Google Drive to access datasets\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Preperation environment for Colab + PySpark\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n","!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n","!pip install -q findspark\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n","spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"id":"A_Nh2Pqh6VG5","executionInfo":{"status":"ok","timestamp":1696896533510,"user_tz":-780,"elapsed":58405,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"fedc1ab0-6958-463b-aecf-7fe6f81d8cf2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x78213b685bd0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://539f75045fe4:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["\"\"\"\n","2. Data understanding\n","2.1. Collecting Initial Data\n","\"\"\"\n","#!pip install yfinance\n","import yfinance as yf\n","import pandas as pd\n","\n","# Source is from internet - Nasdaq 100 companies from Wikipedia\n","source_url = \"https://en.wikipedia.org/wiki/Nasdaq-100\"\n","source_url_tables = pd.read_html(source_url)\n","source_url_onetable = source_url_tables[4]\n","ticker_list = source_url_onetable['Ticker'].to_list()\n"],"metadata":{"id":"MKo67snr6bn-","executionInfo":{"status":"ok","timestamp":1696896538006,"user_tz":-780,"elapsed":2,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Define date, collect 1 year data = 365 days\n","from datetime import date\n","from datetime import timedelta\n","end_date = date.today()\n","start_date = end_date - timedelta(days=365 * 6)\n","# Define timestamp\n","previous_30date = pd.to_datetime(end_date - timedelta(days=30), format='%Y-%m-%d')\n","df_list = list() # Use list to save dataframe from each loop\n","# Download data ordered by ticker\n","for ticker in ticker_list:\n","    data = yf.download(ticker, period=\"max\", start=start_date, end=end_date)\n","    data['Ticker'] = ticker\n","    df_list.append(data)\n","# Merge the dataframe row by row and, use Index as the first column\n","df = pd.concat(df_list).reset_index()\n","# Export dataframe to csv\n","df.to_csv(\"nasdaq.csv\")\n"],"metadata":{"id":"8wFpnEL26soL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","2.2. Describing Data\n","\"\"\"\n","# Schemas can only be inferred for CSV files.\n","df = spark.read.csv('nasdaq.csv', inferSchema=True, header=True)\n","df.printSchema()"],"metadata":{"id":"bjv9N2cR7HFG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's get a better look at the data.\n","# We know that we can show a DataFrame, but that's resulted in a mess!\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pO2Vymg89hf4","executionInfo":{"status":"ok","timestamp":1696901149448,"user_tz":-780,"elapsed":351,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"d319b24e-9aa7-4a36-b7bd-56ac95213f36"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+----------+------------------+------------------+------------------+------------------+------------------+--------+------+\n","|_c0|      Date|              Open|              High|               Low|             Close|         Adj Close|  Volume|Ticker|\n","+---+----------+------------------+------------------+------------------+------------------+------------------+--------+------+\n","|  0|2017-10-11|152.08999633789062|             154.0|151.35000610351562|153.64999389648438|153.64999389648438| 2791000|  ADBE|\n","|  1|2017-10-12|153.72000122070312|154.85000610351562|153.38999938964844|153.61000061035156|153.61000061035156| 2430000|  ADBE|\n","|  2|2017-10-13| 153.9600067138672|154.58999633789062|153.05999755859375|153.92999267578125|153.92999267578125| 2515600|  ADBE|\n","|  3|2017-10-16|151.14999389648438|153.07000732421875|149.27999877929688| 150.4600067138672| 150.4600067138672| 3833200|  ADBE|\n","|  4|2017-10-17|150.52000427246094|150.92999267578125|148.14999389648438| 150.3800048828125| 150.3800048828125| 3547800|  ADBE|\n","|  5|2017-10-18| 150.3300018310547|153.42999267578125|149.02999877929688|             153.0|             153.0| 4290900|  ADBE|\n","|  6|2017-10-19|             165.5|172.14999389648438| 164.4199981689453|171.72999572753906|171.72999572753906|13522600|  ADBE|\n","|  7|2017-10-20|             171.5| 175.8699951171875|171.36000061035156|175.63999938964844|175.63999938964844| 6019400|  ADBE|\n","|  8|2017-10-23|175.67999267578125|175.85000610351562| 171.8300018310547|172.16000366210938|172.16000366210938| 3220300|  ADBE|\n","|  9|2017-10-24|171.47000122070312|172.49000549316406|170.49000549316406| 171.5800018310547| 171.5800018310547| 2658600|  ADBE|\n","| 10|2017-10-25| 170.3699951171875|173.41000366210938| 170.0399932861328| 171.8300018310547| 171.8300018310547| 2785700|  ADBE|\n","| 11|2017-10-26|172.05999755859375|175.02000427246094|172.02000427246094|            173.75|            173.75| 2581200|  ADBE|\n","| 12|2017-10-27|174.02000427246094| 177.5800018310547|174.02000427246094| 177.3300018310547| 177.3300018310547| 2806400|  ADBE|\n","| 13|2017-10-30|             177.0|177.47999572753906| 174.4499969482422|176.02999877929688|176.02999877929688| 2124200|  ADBE|\n","| 14|2017-10-31|176.16000366210938|176.72999572753906|174.52000427246094|175.16000366210938|175.16000366210938| 2656500|  ADBE|\n","| 15|2017-11-01|176.58999633789062|176.94000244140625| 174.6999969482422|            176.25|            176.25| 2002900|  ADBE|\n","| 16|2017-11-02|178.02000427246094|181.47999572753906| 175.8000030517578|180.94000244140625|180.94000244140625| 3262300|  ADBE|\n","| 17|2017-11-03|181.75999450683594|182.74000549316406|179.63999938964844| 182.3000030517578| 182.3000030517578| 2244700|  ADBE|\n","| 18|2017-11-06|182.00999450683594| 182.2899932861328| 180.5399932861328| 180.8000030517578| 180.8000030517578| 2021900|  ADBE|\n","| 19|2017-11-07|181.22999572753906|181.77999877929688|180.11000061035156| 180.8800048828125| 180.8800048828125| 2544200|  ADBE|\n","+---+----------+------------------+------------------+------------------+------------------+------------------+--------+------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["# Instead, let's just grab the first row. Much neater!\n","df.head(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RrKeEoLO91Qw","executionInfo":{"status":"ok","timestamp":1696897392357,"user_tz":-780,"elapsed":870,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"bbc69267-397e-499c-d006-fa5209ccc097"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(_c0=0, Date='2017-10-11', Open=152.08999633789062, High=154.0, Low=151.35000610351562, Close=153.64999389648438, Adj Close=153.64999389648438, Volume=2791000, Ticker='ADBE')]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Let's get a better look at the data from other perspective, like how many rows (count=145492), max and min for the Close price.\n","df.describe().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JfL1ISGJPs8","executionInfo":{"status":"ok","timestamp":1696900389511,"user_tz":-780,"elapsed":3859,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"43f197ae-6081-420c-f1bf-48a149e5cd46"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+----------------+----------+------------------+------------------+------------------+------------------+------------------+--------------------+------+\n","|summary|             _c0|      Date|              Open|              High|               Low|             Close|         Adj Close|              Volume|Ticker|\n","+-------+----------------+----------+------------------+------------------+------------------+------------------+------------------+--------------------+------+\n","|  count|          145492|    145492|            145492|            145492|            145492|            145492|            145492|              145492|145492|\n","|   mean|         72745.5|      null| 178.8071887418212|181.30678334575651|176.26222506107905|178.83487157117764|175.85999965906484|1.0784285464121738E7|  null|\n","| stddev|42000.0670197878|      null| 258.0155303436888| 261.4380846861706|254.51924366303192| 257.9917186679015| 258.1051181763564|2.4557877943180673E7|  null|\n","|    min|               0|2017-10-11|1.2799999713897705|1.2899999618530273|1.2400000095367432|1.2599999904632568|1.2599999904632568|                   0|  AAPL|\n","|    max|          145491|2023-10-09| 3251.469970703125|   3251.7099609375| 3204.989990234375| 3243.010009765625| 3243.010009765625|           914082000|    ZS|\n","+-------+----------------+----------+------------------+------------------+------------------+------------------+------------------+--------------------+------+\n","\n"]}]},{"cell_type":"code","source":["# We can also use Python within the DataFrame filter method, have a look on few rows when AAPL did break through the 150 price.\n","df.filter((df['Open'] < 150) & (df['Close'] > 150) & (df['Ticker'] == 'AAPL')).select('Open','Close','High','Low').show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4sKOOElO-Fms","executionInfo":{"status":"ok","timestamp":1696897891028,"user_tz":-780,"elapsed":928,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"7802f743-d1ef-4940-ba69-f81f27f1dcff"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+------------------+------------------+------------------+\n","|              Open|             Close|              High|               Low|\n","+------------------+------------------+------------------+------------------+\n","| 148.5399932861328| 151.1199951171875|151.19000244140625|146.47000122070312|\n","|             149.0| 153.1199951171875|153.49000549316406|148.61000061035156|\n","|149.82000732421875|152.57000732421875| 153.1699981689453|149.72000122070312|\n","|148.66000366210938|150.02000427246094|151.57000732421875|148.64999389648438|\n","|149.94000244140625|             151.0|151.49000549316406|149.33999633789062|\n","| 147.8300018310547| 151.2100067138672|151.27000427246094|146.86000061035156|\n","|149.77999877929688| 150.1699981689453|150.86000061035156| 148.1999969482422|\n","| 147.9199981689453|             151.0|151.22999572753906|146.91000366210938|\n","|149.30999755859375|154.47999572753906|154.55999755859375|149.10000610351562|\n","|149.66000366210938|150.77000427246094|153.77000427246094|149.63999938964844|\n","| 148.1999969482422|155.74000549316406|             157.5|147.82000732421875|\n","|146.42999267578125|150.72000122070312|151.47999572753906|146.14999389648438|\n","| 148.1300048828125|150.17999267578125| 150.4199981689453|146.92999267578125|\n","| 149.4499969482422|151.07000732421875| 151.8300018310547|149.33999633789062|\n","|148.89999389648438|150.82000732421875|151.17999267578125| 148.1699981689453|\n","|148.02999877929688|             154.5| 157.3800048828125| 147.8300018310547|\n","| 149.4600067138672|151.00999450683594|151.33999633789062|149.22000122070312|\n","| 148.0399932861328|151.02999877929688|151.11000061035156| 147.3300018310547|\n","|147.80999755859375|150.47000122070312|153.13999938964844| 147.6999969482422|\n","+------------------+------------------+------------------+------------------+\n","\n"]}]},{"cell_type":"code","source":["# Let's pick a row of data for AAPL with a low of $157.5 and collect it.\n","employeeResult = df.filter((df['High'] == 157.5) & (df['Ticker'] == 'AAPL')).collect()"],"metadata":{"id":"sNklQ3H__ig7","executionInfo":{"status":"ok","timestamp":1696897968555,"user_tz":-780,"elapsed":833,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# When we collect it, you may notice an interesting format.\n","employeeResult"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZ4sUYaR_ksL","executionInfo":{"status":"ok","timestamp":1696897971428,"user_tz":-780,"elapsed":524,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"d759b875-5896-4813-d12e-6236cc9ba408"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(_c0=18570, Date='2022-10-28', Open=148.1999969482422, High=157.5, Low=147.82000732421875, Close=155.74000549316406, Adj Close=154.82154846191406, Volume=164762400, Ticker='AAPL')]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# We can select the first row of data to shed the outer brackets.\n","employeeRow = employeeResult[0]\n","\n","employeeRow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vwmVsE9OAGVx","executionInfo":{"status":"ok","timestamp":1696898013924,"user_tz":-780,"elapsed":305,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"b3db7c22-706e-4e85-bbd1-f83bde586285"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Row(_c0=18570, Date='2022-10-28', Open=148.1999969482422, High=157.5, Low=147.82000732421875, Close=155.74000549316406, Adj Close=154.82154846191406, Volume=164762400, Ticker='AAPL')"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# And then visualise it simply as a dictionary.\n","employeeRow.asDict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"by3OYE3lAU61","executionInfo":{"status":"ok","timestamp":1696898046151,"user_tz":-780,"elapsed":3,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"0097954c-6d2a-44ab-b456-83bb9c7af151"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'_c0': 18570,\n"," 'Date': '2022-10-28',\n"," 'Open': 148.1999969482422,\n"," 'High': 157.5,\n"," 'Low': 147.82000732421875,\n"," 'Close': 155.74000549316406,\n"," 'Adj Close': 154.82154846191406,\n"," 'Volume': 164762400,\n"," 'Ticker': 'AAPL'}"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["# Why convert it into a dictionary? Because dictionaries have a lot of methods available.\n","# For example, we can simply call volume from the dictionary.\n","employeeRow.asDict()['Volume']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_pQXlMNAaLS","executionInfo":{"status":"ok","timestamp":1696898067323,"user_tz":-780,"elapsed":2,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"96123ce2-7299-42a3-90ad-adbe5739fcfa"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["164762400"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["# Let's import the relevant functions.\n","from pyspark.sql.functions import dayofmonth,month,hour,year,format_number"],"metadata":{"id":"GZy2PaqQAdZG","executionInfo":{"status":"ok","timestamp":1696898080740,"user_tz":-780,"elapsed":515,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# And create a new column using the year function to manipulate date.\n","df_with_year = df.withColumn(\"Year\",year(df[\"Date\"]))\n","\n","df_with_year.head(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTFMvWpZAf9n","executionInfo":{"status":"ok","timestamp":1696901227838,"user_tz":-780,"elapsed":469,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"5c1ae945-d1ae-4c90-d283-b7a26d566fb3"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(_c0=0, Date='2017-10-11', Open=152.08999633789062, High=154.0, Low=151.35000610351562, Close=153.64999389648438, Adj Close=153.64999389648438, Volume=2791000, Ticker='ADBE', Year=2017)]"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["# Now let's sumamrise the data by year, find the mean of each year and select the two columns we'd like to see.\n","df_summary = df_with_year.groupBy(\"Year\").mean().select(['Year','avg(Close)'])\n","df_summary.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fUNdHCZwAubj","executionInfo":{"status":"ok","timestamp":1696898155133,"user_tz":-780,"elapsed":4537,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"b2ea9bb4-11ad-45c3-c2c0-15207afcdcdb"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+------------------+\n","|Year|        avg(Close)|\n","+----+------------------+\n","|2018|123.54981350337941|\n","|2023|224.49420418284572|\n","|2022| 200.9489254671644|\n","|2019| 133.9607567078729|\n","|2020|168.76878581905427|\n","|2017|  113.061506074052|\n","|2021|238.16316005920643|\n","+----+------------------+\n","\n"]}]},{"cell_type":"code","source":["# To make it more visually appealing, let's format the mean to two decimal places.\n","df_formatted = df_summary.select(['Year', format_number(\"avg(Close)\",2)])\n","df_formatted.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2S_UUWtA8Cx","executionInfo":{"status":"ok","timestamp":1696898208544,"user_tz":-780,"elapsed":2720,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"cb3a6b08-6e67-4c0b-d19f-6c3bab113d00"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+----------------------------+\n","|Year|format_number(avg(Close), 2)|\n","+----+----------------------------+\n","|2018|                      123.55|\n","|2023|                      224.49|\n","|2022|                      200.95|\n","|2019|                      133.96|\n","|2020|                      168.77|\n","|2017|                      113.06|\n","|2021|                      238.16|\n","+----+----------------------------+\n","\n"]}]},{"cell_type":"code","source":["# Let's change the name of the column to something that makes sense.\n","df_renamed = df_formatted.withColumnRenamed(\"format_number(avg(Close), 2)\",\"Average Closing Price\")\n","df_renamed.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9V9lT6F0BHIt","executionInfo":{"status":"ok","timestamp":1696901199315,"user_tz":-780,"elapsed":1862,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"cd75e215-2072-496e-d20d-7d8587383f97"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+---------------------+\n","|Year|Average Closing Price|\n","+----+---------------------+\n","|2018|               123.55|\n","|2023|               224.49|\n","|2022|               200.95|\n","|2019|               133.96|\n","|2020|               168.77|\n","|2017|               113.06|\n","|2021|               238.16|\n","+----+---------------------+\n","\n"]}]},{"cell_type":"code","source":["# And finally order it by year.\n","df_renamed.orderBy('Year').show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVXbTkNPBKg7","executionInfo":{"status":"ok","timestamp":1696901194423,"user_tz":-780,"elapsed":2197,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"1dd40501-6ef2-4cee-9dae-d0d75e781188"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+---------------------+\n","|Year|Average Closing Price|\n","+----+---------------------+\n","|2017|               113.06|\n","|2018|               123.55|\n","|2019|               133.96|\n","|2020|               168.77|\n","|2021|               238.16|\n","|2022|               200.95|\n","|2023|               224.49|\n","+----+---------------------+\n","\n"]}]},{"cell_type":"code","source":["# We can also group, and show the averages of each group, let's see which company on average worth the most money.\n","df.groupBy('Ticker').mean().orderBy('avg(Volume)', ascending=False).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-eCt6N4Lmfn","executionInfo":{"status":"ok","timestamp":1696901005592,"user_tz":-780,"elapsed":3036,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"7e18c108-0dd0-4594-e3ef-73a8ad372d90"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------+------------------+------------------+------------------+------------------+------------------+--------------------+\n","|Ticker|avg(_c0)|         avg(Open)|         avg(High)|          avg(Low)|        avg(Close)|    avg(Adj Close)|         avg(Volume)|\n","+------+--------+------------------+------------------+------------------+------------------+------------------+--------------------+\n","|  TSLA|130145.5| 137.9955623459753|141.13868465474178|134.62149467455615|137.97069846540293|137.97069846540293| 1.321278587984085E8|\n","|  AAPL| 18052.5|104.95773725484347|106.17488244863657|103.82332218483842|105.05079079180244| 103.5167384084403| 1.093933996438992E8|\n","|  AMZN|  9004.5| 116.9237798746448|118.33910384797923|115.37476767279425| 116.8727095019596| 116.8727095019596|  8.27053654383289E7|\n","|   AMD| 10512.5|63.909641896381935| 65.22427057334535| 62.54358097729063|  63.9053315357441|  63.9053315357441|  6.92286457301061E7|\n","|  NVDA|101691.5|141.92695115921669|144.59982080737856|139.17050381617457|141.99392908005245| 141.7086952712871| 4.808365958222812E7|\n","| GOOGL|  5988.5| 88.06039624505081| 89.06633032791179| 87.10229148915339| 88.10742307910868| 88.10742307910868|3.4897611885278516E7|\n","|  INTC| 71053.5| 47.40045753398056|47.997440374814545| 46.81477451324463| 47.41002645543146| 43.30472056745534|  3.16573122831565E7|\n","|  GOOG|  7496.5| 88.30977674630972| 89.33160669291367|  87.3886849658875| 88.37796641534456| 88.37796641534456| 3.026369725596817E7|\n","|  MSFT| 94442.5| 203.3388196555626|205.46112071171365|201.14326907595526| 203.3960277951997|198.88081476226725|2.9639542314986736E7|\n","|  LCID| 82748.0|18.058849129472193|18.781840062110415|17.344344599814345|18.053812697277888|18.053812697277888|2.9504007145643692E7|\n","|    MU| 92934.5|57.990968175528856| 58.97748011809129| 56.98017574431726| 57.98417115274728| 57.23260264434612| 2.542961271153846E7|\n","|  META| 89918.5|219.34216179076176|222.40799083254382|216.45255314450048|219.46639925099177|219.46639925099177| 2.405706195159151E7|\n","|  CSCO| 37656.5| 47.84165784092101| 48.29779843752833| 47.39116706898737|47.856564959417284| 43.71065279912569| 2.157559229973475E7|\n","|  SIRI|124113.5| 5.954442969842046| 6.032453580307392| 5.881657834552644|5.9536405811259225| 5.559966646075565|2.1298861655172415E7|\n","| CMCSA| 40672.5| 42.70135937698324| 43.16198820144491| 42.23381629356971| 42.70497345102245|  39.7017811350228|2.1099300054376658E7|\n","|   CSX| 48227.5|27.243945589748556| 27.54061118487654|26.960022085225233|27.248229413829367|  26.3155671766013|1.4614499807029178E7|\n","|    JD| 75577.5| 50.71531432503414| 51.57345030099074| 49.77345422977478| 50.67865384605266|49.009053283408086|1.3061853826923076E7|\n","|  PYPL|113755.5|128.75758017785353|130.53418894598275|126.72015017856022|128.66337212074342|128.66337212074342|1.0786031584880637E7|\n","|  QCOM|118081.5|104.28574269646359|105.76991384541641|102.80230107041822| 104.2956697656242| 98.14238146318998|1.0678225929708222E7|\n","|   WBD|139193.5|24.327785156766048| 24.81150198551957| 23.84050068741452|24.327533155916857|24.327533155916857|1.0310154162466843E7|\n","+------+--------+------------------+------------------+------------------+------------------+------------------+--------------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["# Other than grouping by Ticker, we can group by Year and use aggregation which represents the entire dataset.\n","df_with_year.groupBy('Year').agg({\"Close\":\"mean\"}).orderBy('avg(Close)', ascending=False).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjTsysJ8LxPj","executionInfo":{"status":"ok","timestamp":1696901426509,"user_tz":-780,"elapsed":2196,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"39c94e6e-7cef-4851-a248-59f4cc77053c"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+------------------+\n","|Year|        avg(Close)|\n","+----+------------------+\n","|2021|238.16316005920643|\n","|2023|224.49420418284572|\n","|2022| 200.9489254671644|\n","|2020|168.76878581905427|\n","|2019| 133.9607567078729|\n","|2018|123.54981350337941|\n","|2017|  113.061506074052|\n","+----+------------------+\n","\n"]}]},{"cell_type":"code","source":["# We can also import SQL functions.\n","from pyspark.sql.functions import countDistinct,avg,stddev,format_number\n","df.select(avg('Close').alias('Average Close')).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DdMl2m7_SpdL","executionInfo":{"status":"ok","timestamp":1696902974397,"user_tz":-780,"elapsed":999,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"56f2e54d-98af-404e-95da-3cf9095a8d26"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+\n","|     Average Close|\n","+------------------+\n","|178.83487157117764|\n","+------------------+\n","\n"]}]},{"cell_type":"code","source":["# We can use SQL to do some basic manipulation.\n","# Here we're getting the standard deviation in sales, formatting it to two decimal places and changing the column name.\n","dev = df.select(stddev(\"Close\"))\n","\n","dev.select(format_number('stddev_samp(Close)',2).alias('Close_Standard_Deviation')).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QyJ4nsPbTNsY","executionInfo":{"status":"ok","timestamp":1696903012199,"user_tz":-780,"elapsed":1106,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"971bac8e-7751-4613-a0e8-ed3d12cf893f"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------+\n","|Close_Standard_Deviation|\n","+------------------------+\n","|                  257.99|\n","+------------------------+\n","\n"]}]},{"cell_type":"code","source":["\"\"\"\n","LinearRegression\n","\"\"\"\n","# Must be included at the beginning of each new notebook. Remember to change the app name.\n","import findspark\n","findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n","import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('linear_regression_adv').getOrCreate()\n","\n","# If you're getting an error with numpy, please type 'sudo pip install numpy --user' into the EC2 console.\n","from pyspark.ml.regression import LinearRegression\n"],"metadata":{"id":"hqWIuZzkXOYb","executionInfo":{"status":"ok","timestamp":1696908225788,"user_tz":-780,"elapsed":331,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":109,"outputs":[]},{"cell_type":"code","source":["# Use Spark to read in the Ecommerce Customers csv file. You can infer csv schemas.\n","data = spark.read.csv(\"nasdaq.csv\",inferSchema=True,header=True)\n","\n","# Print the schema of the DataFrame. You can see potential features as well as the predictor - String is not supported.\n","data.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-sySUZVnJg0","executionInfo":{"status":"ok","timestamp":1696908229399,"user_tz":-780,"elapsed":1273,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"7580a080-7f89-4fd9-c6f4-aeb8e98cdf2a"},"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- _c0: integer (nullable = true)\n"," |-- Date: string (nullable = true)\n"," |-- Open: double (nullable = true)\n"," |-- High: double (nullable = true)\n"," |-- Low: double (nullable = true)\n"," |-- Close: double (nullable = true)\n"," |-- Adj Close: double (nullable = true)\n"," |-- Volume: integer (nullable = true)\n"," |-- Ticker: string (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["# A simple for loop allows us to give an example what column values that each rows contain.\n","for item in data.head():\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R7A5hO5eanpJ","executionInfo":{"status":"ok","timestamp":1696905340214,"user_tz":-780,"elapsed":833,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"3b6d49e0-6d70-4853-9fa1-cdc627953a18"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","2017-10-11\n","152.08999633789062\n","154.0\n","151.35000610351562\n","153.64999389648438\n","153.64999389648438\n","2791000\n","ADBE\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Setting Up a DataFrame for Machine Learning (MLlib)\n","We need to do a few things before Spark can accept the data for machine learning. First of all, it needs to be in the form of two columns: label and features. Unlike the documentation example, this data is messy. We'll need to combine all of the features into a single vector. VectorAssembler simplifies the process.\n","\"\"\"\n","# Import VectorAssembler and Vectors\n","from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import VectorAssembler\n","\n","# The input columns are the feature column names, and the output column is what you'd like the new column to be named.\n","assembler = VectorAssembler(\n","    inputCols=[\"Open\", \"High\", \"Low\",\n","               \"Volume\"],\n","    outputCol=\"features\")\n"],"metadata":{"id":"I0G-Weoccbi2","executionInfo":{"status":"ok","timestamp":1696906523898,"user_tz":-780,"elapsed":466,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["# Now that we've created the assembler variable, let's actually transform the data.\n","output = assembler.transform(data)\n","\n","# Using print schema, you see that the features output column has been added.\n","output.printSchema()\n","\n","# You can see that the features column is a dense vector that combines the various features as expected.\n","output.head(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQts_p_TdYJN","executionInfo":{"status":"ok","timestamp":1696906526873,"user_tz":-780,"elapsed":461,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"93f4a25b-aee8-4a81-d8fa-368bf77f4e74"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- _c0: integer (nullable = true)\n"," |-- Date: string (nullable = true)\n"," |-- Open: double (nullable = true)\n"," |-- High: double (nullable = true)\n"," |-- Low: double (nullable = true)\n"," |-- Close: double (nullable = true)\n"," |-- Adj Close: double (nullable = true)\n"," |-- Volume: integer (nullable = true)\n"," |-- Ticker: string (nullable = true)\n"," |-- features: vector (nullable = true)\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["[Row(_c0=0, Date='2017-10-11', Open=152.08999633789062, High=154.0, Low=151.35000610351562, Close=153.64999389648438, Adj Close=153.64999389648438, Volume=2791000, Ticker='ADBE', features=DenseVector([152.09, 154.0, 151.35, 2791000.0]))]"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["# Let's select two columns (the feature and predictor).\n","# This is now in the appropriate format to be processed by Spark.\n","final_data = output.select(\"features\",'Close')\n","final_data.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aHb5-0e8eNtB","executionInfo":{"status":"ok","timestamp":1696906536617,"user_tz":-780,"elapsed":449,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"d4a4c33f-4140-4e03-cd8d-d10d50a0c68c"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+------------------+\n","|            features|             Close|\n","+--------------------+------------------+\n","|[152.089996337890...|153.64999389648438|\n","|[153.720001220703...|153.61000061035156|\n","|[153.960006713867...|153.92999267578125|\n","|[151.149993896484...| 150.4600067138672|\n","|[150.520004272460...| 150.3800048828125|\n","|[150.330001831054...|             153.0|\n","|[165.5,172.149993...|171.72999572753906|\n","|[171.5,175.869995...|175.63999938964844|\n","|[175.679992675781...|172.16000366210938|\n","|[171.470001220703...| 171.5800018310547|\n","|[170.369995117187...| 171.8300018310547|\n","|[172.059997558593...|            173.75|\n","|[174.020004272460...| 177.3300018310547|\n","|[177.0,177.479995...|176.02999877929688|\n","|[176.160003662109...|175.16000366210938|\n","|[176.589996337890...|            176.25|\n","|[178.020004272460...|180.94000244140625|\n","|[181.759994506835...| 182.3000030517578|\n","|[182.009994506835...| 180.8000030517578|\n","|[181.229995727539...| 180.8800048828125|\n","+--------------------+------------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["# Data Partioning: Split the training and testing set and verify.\n","train_data,test_data = final_data.randomSplit([0.7,0.3])\n","train_data.describe().show()\n","test_data.describe().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mNBMQA-fekbh","executionInfo":{"status":"ok","timestamp":1696911191421,"user_tz":-780,"elapsed":1602,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"60a32099-7eca-45fb-fc93-e087eb0ed02a"},"execution_count":159,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+------------------+\n","|summary|             Close|\n","+-------+------------------+\n","|  count|             26757|\n","|   mean|231.35819262774282|\n","| stddev|311.98979676343623|\n","|    min|  5.03000020980835|\n","|    max|    3234.330078125|\n","+-------+------------------+\n","\n","+-------+------------------+\n","|summary|             Close|\n","+-------+------------------+\n","|  count|             11388|\n","|   mean|238.39748356716652|\n","| stddev| 335.9351834119386|\n","|    min| 5.130000114440918|\n","|    max| 3243.010009765625|\n","+-------+------------------+\n","\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Now we can create a Linear Regression Model object. Because the feature column is named 'features',\n","we don't have to worry about it. However, as the labelCol isn't the default name, we have to specify it's name (Close).\n","\"\"\"\n","lr = LinearRegression(labelCol='Close')\n","\n","# Fit the model to the data.\n","lrModel = lr.fit(train_data)\n","\n","# Print the coefficients and intercept for linear regression.\n","print(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6zvME07xfppE","executionInfo":{"status":"ok","timestamp":1696906553563,"user_tz":-780,"elapsed":5305,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"9e35ff38-ed19-484d-d6e4-9c396c2152a9"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["Coefficients: [-0.5835194810999815,0.8001081043837726,0.7833741653000084,1.7922392355944063e-10] Intercept: 0.03374444176056932\n"]}]},{"cell_type":"code","source":["# Let's evaluate the model against the test data.\n","test_results = lrModel.evaluate(test_data)\n","\n","# Interesting results! This shows the difference between the predicted value and the test data.\n","test_results.residuals.show()\n","\n","# Let's get some evaluation metrics (as discussed in the previous linear regression notebook).\n","print(\"RSME: {}\".format(test_results.rootMeanSquaredError))"],"metadata":{"id":"kpQzrFEef-Xx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We can also get the R2 value.\n","print(\"R2: {}\".format(test_results.r2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ati9G1JFgFy3","executionInfo":{"status":"ok","timestamp":1696906563447,"user_tz":-780,"elapsed":373,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"e9748b68-c247-4063-acb6-3aabf089dc72"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["R2: 0.9998966524807389\n"]}]},{"cell_type":"markdown","source":["Conclusion:\n","\n","Looking at RMSE and R2, we can see that the model is quite accurate. The RMSE shows that, on average, there's only around 2.65 price discrepancy between the actual and predicted results. Comparing this to the table below, the average close (178.8) and standard deviation (257.99), a 2.65 error is surprisingly good.\n","\n","The R2 also shows that the model accounts for 99.989% of the variance in the data."],"metadata":{"id":"FrAaaaK0hlgU"}},{"cell_type":"code","source":["final_data.describe().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8K_MZf2FhOd9","executionInfo":{"status":"ok","timestamp":1696906671329,"user_tz":-780,"elapsed":1584,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"9ab8291d-61cf-4926-c2d2-4a0408078f5d"},"execution_count":103,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+------------------+\n","|summary|             Close|\n","+-------+------------------+\n","|  count|            145492|\n","|   mean|178.83487157117764|\n","| stddev| 257.9917186679015|\n","|    min|1.2599999904632568|\n","|    max| 3243.010009765625|\n","+-------+------------------+\n","\n"]}]},{"cell_type":"code","source":["\"\"\"\n","But what if we didn't have the predictor data?\n","This isn't really relevant to your assignment, but useful in a real-world scenario.\n","What if you have all of these features but no predictor data? How do you actually use the model you've created? Check out the example below.\n","\"\"\"\n","# Let's just select the features column (removing the label column).\n","unlabeled_data = test_data.select('features')\n","unlabeled_data.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-r0XeXeiVMb","executionInfo":{"status":"ok","timestamp":1696907442908,"user_tz":-780,"elapsed":1284,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"f74a0b42-78b1-4a52-bc6e-03e6ab2128ef"},"execution_count":107,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+\n","|            features|\n","+--------------------+\n","|[1.37000000476837...|\n","|[1.39999997615814...|\n","|[1.39999997615814...|\n","|[1.5,1.5299999713...|\n","|[1.89999997615814...|\n","|[2.02999997138977...|\n","|[2.06999993324279...|\n","|[2.07999992370605...|\n","|[2.09999990463256...|\n","|[2.09999990463256...|\n","|[2.16000008583068...|\n","|[2.23000001907348...|\n","|[2.31999993324279...|\n","|[2.36999988555908...|\n","|[2.38000011444091...|\n","|[2.40000009536743...|\n","|[2.5,2.6900000572...|\n","|[2.61999988555908...|\n","|[2.65000009536743...|\n","|[2.67000007629394...|\n","+--------------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Useful for visualizing the predition data later on\n","\"\"\"\n","# Now we can transform the unlabeled data.\n","predictions = lrModel.transform(unlabeled_data)\n","\n","# It worked! Feeding the unlabeled data features into the model results in a prediction,\n","# which is also likely to be the \"Close\" price.\n","predictions.show()\n","predictions.head(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tE25n8NZiZQe","executionInfo":{"status":"ok","timestamp":1696907455721,"user_tz":-780,"elapsed":2397,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"a42d886a-ee25-4270-a4c2-dd61cd008e95"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+------------------+\n","|            features|        prediction|\n","+--------------------+------------------+\n","|[1.37000000476837...|1.3569023071146038|\n","|[1.39999997615814...|1.4185826278289644|\n","|[1.39999997615814...|1.4662686797620015|\n","|[1.5,1.5299999713...|1.5342861686812765|\n","|[1.89999997615814...| 2.046345814094968|\n","|[2.02999997138977...| 2.025075523344195|\n","|[2.06999993324279...| 2.159915926835926|\n","|[2.07999992370605...|2.0121529937350506|\n","|[2.09999990463256...| 2.016296919621521|\n","|[2.09999990463256...|2.2378677082473506|\n","|[2.16000008583068...| 2.084259981675241|\n","|[2.23000001907348...| 2.361236004439006|\n","|[2.31999993324279...|2.4185254399764817|\n","|[2.36999988555908...| 2.637034214927953|\n","|[2.38000011444091...| 2.296285537722992|\n","|[2.40000009536743...|2.4346616433904016|\n","|[2.5,2.6900000572...|2.6074871407280833|\n","|[2.61999988555908...| 2.504904009050538|\n","|[2.65000009536743...| 2.867816174665039|\n","|[2.67000007629394...|2.8414939198843596|\n","+--------------------+------------------+\n","only showing top 20 rows\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["[Row(features=DenseVector([1.37, 1.38, 1.3, 245500.0]), prediction=1.3569023071146038)]"]},"metadata":{},"execution_count":108}]},{"cell_type":"code","source":["\"\"\"\n","LogisticRegression\n","\"\"\"\n","# Must be included at the beginning of each new notebook. Remember to change the app name.\n","import findspark\n","findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n","import pyspark\n","from pyspark.sql import *\n","spark = SparkSession.builder.appName('logistic_regression_adv').getOrCreate()\n","\n","# If you're getting an error with numpy, please type 'sudo pip install numpy --user' into the EC2 console.\n","from pyspark.ml.classification import LogisticRegression"],"metadata":{"id":"NPq_3EKEnQp0","executionInfo":{"status":"ok","timestamp":1696908301946,"user_tz":-780,"elapsed":461,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Amend Dataset with 0/1 predictor\n","\"\"\"\n","#!pip install yfinance\n","import yfinance as yf\n","import pandas as pd\n","\n","# Source is from internet - Nasdaq 100 companies from Wikipedia\n","source_url = \"https://en.wikipedia.org/wiki/Nasdaq-100\"\n","source_url_tables = pd.read_html(source_url)\n","source_url_onetable = source_url_tables[4]\n","ticker_list = source_url_onetable['Ticker'].to_list()\n","\n","# Start financial data download\n","from datetime import date\n","from datetime import timedelta\n","\n","# Define date, collect 1 year data = 365 days\n","end_date = date.today()\n","start_date = end_date - timedelta(days=365 * 2)\n","\n","# Define timestamp\n","previous_30date = pd.to_datetime(end_date - timedelta(days=30), format='%Y-%m-%d')\n","\n","df_list = list() # Use list to save dataframe from each loop\n","# Download data ordered by ticker\n","for ticker in ticker_list:\n","    data = yf.download(ticker, period=\"max\", start=start_date, end=end_date)\n","    # data.insert(0, 'Ticker', ticker) # To add ticker to (first) column right after index column\n","    data['Ticker'] = ticker\n","\n","    # Initialize 'Win_Count' and 'Win_Rate'\n","    data['Win_Count'] = 0\n","    data['Win_Loss'] = \"\"\n","\n","    # Cal ROI within the future 30 days respectively， add and save in new columns, shift() to rows up and diff\n","    for future_days in range(30):\n","        data[f\"ROI_T+{future_days}\"] = round(data['High'].shift(-future_days) / data['Open'] - 1, 7)\n","        # Count and top up 'Win_Count' when there's matched ROI >= 0.08\n","        data.loc[data[f\"ROI_T+{future_days}\"] >= 0.08, 'Win_Count'] = data['Win_Count'] + 1\n","\n","    # Calculate the past MA, need to shift(past_days) rows down\n","    data['MA_50'] = 0\n","    for past_days in range(50):\n","        data['MA_50'] = data['MA_50'] + data['Close'].shift(past_days)\n","\n","    data['MA_100'] = 0\n","    for past_days in range(100):\n","        data['MA_100'] = data['MA_100'] + data['Close'].shift(past_days)\n","\n","    data['MA_200'] = 0\n","    for past_days in range(200):\n","        data['MA_200'] = data['MA_200'] + data['Close'].shift(past_days)\n","\n","    df_list.append(data)\n","# Merge the dataframe row by row and, use Index as the first column, if column by column pls specify \"axis=1\"\n","df = pd.concat(df_list).reset_index()\n","\n","# Basic Stats calculation and adding more attributes\n","df['MA_50'] = round(df['MA_50'] / 50, 3)\n","df['MA_100'] = round(df['MA_100'] / 100, 3)\n","df['MA_200'] = round(df['MA_200'] / 200, 3)\n","\n","# Assign 'Win_Loss' and MA based on the condition from basic stats calculation\n","df.loc[df['Win_Count'] > 0, 'Win_Loss'] = \"0\"\n","df.loc[df['Win_Count'] == 0, 'Win_Loss'] = \"1\"\n","# 'Win_Loss' is still unknown, since no Win so far however the 30 days deadline is not dued yet\n","df.loc[(df['Win_Count'] == 0) & (df['Date'] > previous_30date), 'Win_Loss'] = \"\"\n","\n","df['Yin_Yang'] = \"\"\n","df.loc[df['Open'] > df['Close'], 'Yin_Yang'] = \"Yin\"\n","df.loc[df['Open'] < df['Close'], 'Yin_Yang'] = \"Yang\"\n","df.loc[df['Open'] == df['Close'], 'Yin_Yang'] = \"Ping\"\n","\n","df['Fluctuation'] = round(df['High'] / df['Low'] - 1, 3)\n","\n","# Export dataframe to csv\n","df.to_csv(\"nasdaq.csv\")"],"metadata":{"id":"k6WW8nmlqheS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read in the CSV data.\n","data = spark.read.csv('nasdaq.csv',inferSchema=True,header=True)\n","\n","# Print data schema.\n","data.printSchema()"],"metadata":{"id":"dOfjCtownfup"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select a list of relevant columns.\n","# Name, for example, is somewhat irrelevant.\n","my_cols = data.select(['Date',\n"," 'Open',\n"," 'High',\n"," 'Low',\n"," 'Close',\n"," 'Volume',\n"," 'Ticker',\n"," 'Win_Loss'])\n","\n","# Now that we've selected the relevant columns, let's drop the missing data (Data Cleaning).\n","my_final_data = my_cols.na.drop()\n","my_final_data.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-HR7RwXnuAp","executionInfo":{"status":"ok","timestamp":1696910190405,"user_tz":-780,"elapsed":306,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"1d283dd4-6b5b-42e0-b009-cc525985494f"},"execution_count":143,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- Date: string (nullable = true)\n"," |-- Open: double (nullable = true)\n"," |-- High: double (nullable = true)\n"," |-- Low: double (nullable = true)\n"," |-- Close: double (nullable = true)\n"," |-- Volume: integer (nullable = true)\n"," |-- Ticker: string (nullable = true)\n"," |-- Win_Loss: integer (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["# Working with Categorical Columns - \"Strings\" as the values\n","from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n","                                OneHotEncoder,StringIndexer)\n","\n","# First create a string indexer (convert every string into a number, such as male = 0 and female = 1).\n","# A number will be assigned to every category in the column.\n","date_indexer = StringIndexer(inputCol='Date',outputCol='DateIndex')\n","\n","# Now we can one hot encode these numbers. This converts the various outputs into a single vector.\n","# This makes it easier to process when you have multiple classes.\n","date_encoder = OneHotEncoder(inputCol='DateIndex',outputCol='DateVec')\n","\n","# Similar to the above.\n","ticker_indexer = StringIndexer(inputCol='Ticker',outputCol='TickerIndex')\n","ticker_encoder = OneHotEncoder(inputCol='TickerIndex',outputCol='TickerVec')"],"metadata":{"id":"Edm2mSPCoGDH","executionInfo":{"status":"ok","timestamp":1696910197185,"user_tz":-780,"elapsed":311,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":144,"outputs":[]},{"cell_type":"code","source":["# Now we can assemble all of this as one vector in the features column.\n","assembler = VectorAssembler(inputCols=['DateVec',\n"," 'Open',\n"," 'High',\n"," 'Low',\n"," 'Close',\n"," 'Volume',\n"," 'TickerVec'],outputCol='features')"],"metadata":{"id":"IMjTv41rpB0d","executionInfo":{"status":"ok","timestamp":1696910199470,"user_tz":-780,"elapsed":387,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":145,"outputs":[]},{"cell_type":"code","source":["# A pipeline sets stages for different steps. Let's see an example of how to use pipelines.\n","from pyspark.ml import Pipeline\n","\n","# Note that Win_Loss is a categorial variable but didn't require any transformation.\n","# That's because it's already in the format of 1's and 0's.\n","log_reg_nasdaq = LogisticRegression(featuresCol='features',labelCol='Win_Loss')"],"metadata":{"id":"h88IBVB9peN1","executionInfo":{"status":"ok","timestamp":1696910211051,"user_tz":-780,"elapsed":345,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":148,"outputs":[]},{"cell_type":"code","source":["# Lists everything we want to do. Index data, encode data, assemble data and then pass in the actual model.\n","pipeline = Pipeline(stages=[date_indexer,ticker_indexer,\n","                           date_encoder,ticker_encoder,\n","                           assembler,log_reg_nasdaq])"],"metadata":{"id":"I3F5NKl9rtEv","executionInfo":{"status":"ok","timestamp":1696910213157,"user_tz":-780,"elapsed":305,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":149,"outputs":[]},{"cell_type":"code","source":["# Train/test split.\n","train_nasdaq_data, test_nasdaq_data = my_final_data.randomSplit([0.7,.3])\n","train_nasdaq_data.describe().show()\n","test_nasdaq_data.describe().show()\n","\n","\n","# Note pipeline. Call it as you would call a machine learning object.\n","fit_model = pipeline.fit(train_nasdaq_data)\n","\n","# Transform test data.\n","results = fit_model.transform(test_nasdaq_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HQZlBFLOsdmw","executionInfo":{"status":"ok","timestamp":1696911264828,"user_tz":-780,"elapsed":13390,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"7d3167f0-8091-440e-eff0-13a6b30e879f"},"execution_count":160,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+----------+-----------------+------------------+------------------+------------------+--------------------+------+-------------------+\n","|summary|      Date|             Open|              High|               Low|             Close|              Volume|Ticker|           Win_Loss|\n","+-------+----------+-----------------+------------------+------------------+------------------+--------------------+------+-------------------+\n","|  count|     34029|            34029|             34029|             34029|             34029|               34029| 34029|              34029|\n","|   mean|      null|214.7783421778217|218.04239514813673|211.53340979280892|214.84236586052916|   9934958.727614682|  null| 0.5306650210114902|\n","| stddev|      null|284.5228095344425|288.61287883587534| 280.6751856335386|284.72878395051197|1.9402971760395672E7|  null|0.49906610358023173|\n","|    min|2021-10-11|3.380000114440918|3.4800000190734863| 3.319999933242798|3.4200000762939453|               22200|  AAPL|                  0|\n","|    max|2023-10-06|3251.469970703125| 3251.469970703125|  3195.06005859375| 3243.010009765625|           306590600|    ZS|                  1|\n","+-------+----------+-----------------+------------------+------------------+------------------+--------------------+------+-------------------+\n","\n","+-------+----------+------------------+------------------+------------------+------------------+--------------------+------+-------------------+\n","|summary|      Date|              Open|              High|               Low|             Close|              Volume|Ticker|           Win_Loss|\n","+-------+----------+------------------+------------------+------------------+------------------+--------------------+------+-------------------+\n","|  count|     14314|             14314|             14314|             14314|             14314|               14314| 14314|              14314|\n","|   mean|      null|215.60554600161714|218.89188132879704|212.31732738582684|215.65258067567305| 1.056382340987844E7|  null| 0.5264077127287969|\n","| stddev|      null| 283.9981322358555| 288.1171859471476|279.76748890856936| 283.8766300511293|2.0904407814776666E7|  null|0.49931958766355544|\n","|    min|2021-10-11|3.5299999713897705|3.5399999618530273|3.4700000286102295| 3.509999990463257|                   0|  AAPL|                  0|\n","|    max|2023-10-06|            3215.0|   3251.7099609375| 3204.989990234375|    3234.330078125|           377220900|    ZS|                  1|\n","+-------+----------+------------------+------------------+------------------+------------------+--------------------+------+-------------------+\n","\n"]}]},{"cell_type":"code","source":["# Evaluate the model using the binary classifer.\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n","                                       labelCol='Win_Loss')"],"metadata":{"id":"JOZ-H_cSu5gX","executionInfo":{"status":"ok","timestamp":1696911280600,"user_tz":-780,"elapsed":331,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":161,"outputs":[]},{"cell_type":"code","source":["# If we select the actual and predicted results, we can see that some predictions were correct while others were wrong.\n","results.select('Win_Loss','prediction').show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6IbZS9fZvAst","executionInfo":{"status":"ok","timestamp":1696911283393,"user_tz":-780,"elapsed":811,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"1fa47b63-5fd2-49b1-fe80-305defe9dd84"},"execution_count":162,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+----------+\n","|Win_Loss|prediction|\n","+--------+----------+\n","|       0|       0.0|\n","|       1|       0.0|\n","|       0|       0.0|\n","|       1|       0.0|\n","|       0|       0.0|\n","|       0|       0.0|\n","|       0|       0.0|\n","|       1|       1.0|\n","|       0|       0.0|\n","|       1|       0.0|\n","|       0|       0.0|\n","|       0|       0.0|\n","|       0|       0.0|\n","|       0|       0.0|\n","|       0|       1.0|\n","|       0|       1.0|\n","|       0|       0.0|\n","|       0|       0.0|\n","|       0|       0.0|\n","|       1|       1.0|\n","+--------+----------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["# We can then evaluate using AUC (area under the curve). AUC is linked to ROC.\n","AUC = my_eval.evaluate(results)\n","\n","AUC"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mpyi35NQvHXB","executionInfo":{"status":"ok","timestamp":1696911287362,"user_tz":-780,"elapsed":1524,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"c74cb4bc-adb8-4f86-cfbe-9879640326ff"},"execution_count":163,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7394722450269691"]},"metadata":{},"execution_count":163}]},{"cell_type":"code","source":["# Not all interpretation has to be visualisations. You can also gain a lot of information with text.\n","# For example, here we're seeing how much variance our model could account for.\n","# According to this, the model got 10611/14314 correct.\n","totalResults = results.select('Win_Loss','prediction')\n","\n","correctResults = totalResults.filter(totalResults['Win_Loss'] == totalResults['prediction'])\n","\n","countTR = totalResults.count()\n","print(\"Total Result: \" + str(countTR))\n","\n","countTC = correctResults.count()\n","print(\"Total Correct: \" + str(countTC))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6eZ29NSzxcv9","executionInfo":{"status":"ok","timestamp":1696911291338,"user_tz":-780,"elapsed":2430,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"10ceea6c-e542-4c71-95fd-0085a3e7dec5"},"execution_count":164,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Result: 14314\n","Total Correct: 10611\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Prepare Dataset with Catagorical predictor W/L\n","\"\"\"\n","#!pip install yfinance\n","import yfinance as yf\n","import pandas as pd\n","\n","# Source is from internet - Nasdaq 100 companies from Wikipedia\n","source_url = \"https://en.wikipedia.org/wiki/Nasdaq-100\"\n","source_url_tables = pd.read_html(source_url)\n","source_url_onetable = source_url_tables[4]\n","ticker_list = source_url_onetable['Ticker'].to_list()\n","\n","# Start financial data download\n","from datetime import date\n","from datetime import timedelta\n","import numpy as np\n","\n","# Define date, collect 1 year data = 365 days\n","end_date = date.today()\n","start_date = end_date - timedelta(days=365 * 2)\n","\n","# Define timestamp\n","previous_30date = pd.to_datetime(end_date - timedelta(days=30), format='%Y-%m-%d')\n","\n","df_list = list() # Use list to save dataframe from each loop\n","# Download data ordered by ticker\n","for ticker in ticker_list:\n","    data = yf.download(ticker, period=\"max\", start=start_date, end=end_date)\n","    # data.insert(0, 'Ticker', ticker) # To add ticker to (first) column right after index column\n","    data['Ticker'] = ticker\n","\n","    # Initialize 'Win_Count' and 'Win_Rate'\n","    data['Win_Count'] = 0\n","    data['Win_Loss'] = \"\"\n","\n","    # Cal ROI within the future 30 days respectively， add and save in new columns, shift() to rows up and diff\n","    for future_days in range(30):\n","        data[f\"ROI_T+{future_days}\"] = round(data['High'].shift(-future_days) / data['Open'] - 1, 7)\n","        # Count and top up 'Win_Count' when there's matched ROI >= 0.08\n","        data.loc[data[f\"ROI_T+{future_days}\"] >= 0.08, 'Win_Count'] = data['Win_Count'] + 1\n","\n","    # Calculate the past MA, need to shift(past_days) rows down\n","    data['MA_50'] = 0\n","    for past_days in range(50):\n","        data['MA_50'] = data['MA_50'] + data['Close'].shift(past_days)\n","\n","    data['MA_100'] = 0\n","    for past_days in range(100):\n","        data['MA_100'] = data['MA_100'] + data['Close'].shift(past_days)\n","\n","    data['MA_200'] = 0\n","    for past_days in range(200):\n","        data['MA_200'] = data['MA_200'] + data['Close'].shift(past_days)\n","\n","    df_list.append(data)\n","# Merge the dataframe row by row and, use Index as the first column, if column by column pls specify \"axis=1\"\n","df = pd.concat(df_list).reset_index()\n","\n","# Basic Stats calculation and adding more attributes\n","df['MA_50'] = round(df['MA_50'] / 50, 3)\n","df['MA_100'] = round(df['MA_100'] / 100, 3)\n","df['MA_200'] = round(df['MA_200'] / 200, 3)\n","\n","# Assign 'Win_Loss' and MA based on the condition from basic stats calculation\n","df.loc[df['Win_Count'] > 0, 'Win_Loss'] = \"W\"\n","df.loc[df['Win_Count'] == 0, 'Win_Loss'] = \"L\"\n","# 'Win_Loss' is still unknown, since no Win so far however the 30 days deadline is not dued yet, drop such rows\n","df.loc[(df['Win_Count'] == 0) & (df['Date'] > previous_30date), 'Win_Loss'] = \"\"\n","df['Win_Loss'] = df['Win_Loss'].replace(\"\", np.nan)\n","df = df.dropna()\n","\n","df['Yin_Yang'] = \"\"\n","df.loc[df['Open'] > df['Close'], 'Yin_Yang'] = \"Yin\"\n","df.loc[df['Open'] < df['Close'], 'Yin_Yang'] = \"Yang\"\n","df.loc[df['Open'] == df['Close'], 'Yin_Yang'] = \"Ping\"\n","\n","df['Fluctuation'] = round(df['High'] / df['Low'] - 1, 3)\n","\n","# Export dataframe to csv\n","df.to_csv(\"nasdaq.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6D7JM7w_LYNl","executionInfo":{"status":"ok","timestamp":1696920371691,"user_tz":-780,"elapsed":48212,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"3416cca4-6584-48a8-f27f-483e2d4ee02d"},"execution_count":226,"outputs":[{"output_type":"stream","name":"stdout","text":["[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n","[*********************100%%**********************]  1 of 1 completed\n"]}]},{"cell_type":"code","source":["# Must be included at the beginning of each new notebook. Remember to change the app name.\n","import findspark\n","findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n","import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('tree_methods_adv').getOrCreate()\n","\n","# Read in the CSV data.\n","data = spark.read.csv('nasdaq.csv',inferSchema=True,header=True)"],"metadata":{"id":"AMXaFvTrAAIJ","executionInfo":{"status":"ok","timestamp":1696920380452,"user_tz":-780,"elapsed":647,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":227,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Spark Formatting of Data for Tree Methods\n","\"\"\"\n","# A few things we need to do before Spark can accept the data!\n","# It needs to be in the form of two columns: \"label\" and \"features\".\n","\n","# Import VectorAssembler and Vectors\n","from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import VectorAssembler\n","\n","# Combine all features into one vector named features.\n","assembler = VectorAssembler(\n","  inputCols=['Open',\n","            'High',\n","            'Low',\n","            'Close',\n","            'Volume'],\n","            outputCol='features')"],"metadata":{"id":"p_TMd7HsFqJX","executionInfo":{"status":"ok","timestamp":1696920386690,"user_tz":-780,"elapsed":449,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":228,"outputs":[]},{"cell_type":"code","source":["# Let's transform the data.\n","output = assembler.transform(data)"],"metadata":{"id":"AsMs_O9JGPMu","executionInfo":{"status":"ok","timestamp":1696920390119,"user_tz":-780,"elapsed":314,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":229,"outputs":[]},{"cell_type":"code","source":["# String Handling Part: Import the string indexer (similar to the logistic regression exercises).\n","from pyspark.ml.feature import StringIndexer\n","\n","indexer = StringIndexer(inputCol=\"Win_Loss\", outputCol=\"WinIndex\")\n","output_fixed = indexer.fit(output).transform(output)\n","\n","# Let's select the two columns we want. Features (which contains vectors), and the predictor.\n","final_data = output_fixed.select(\"features\",\"WinIndex\")\n","final_data.show()\n","final_data.describe().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWZQq7IoHbZv","executionInfo":{"status":"ok","timestamp":1696920393946,"user_tz":-780,"elapsed":1029,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"eb85fa61-f0c0-493d-efb3-1fae5b8aa5c7"},"execution_count":230,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+--------+\n","|            features|WinIndex|\n","+--------------------+--------+\n","|[385.079986572265...|     1.0|\n","|[392.760009765625...|     1.0|\n","|[404.690002441406...|     1.0|\n","|[406.510009765625...|     1.0|\n","|[405.339996337890...|     1.0|\n","|[412.809997558593...|     1.0|\n","|[425.450012207031...|     0.0|\n","|[423.0,434.540008...|     0.0|\n","|[435.0,440.299987...|     0.0|\n","|[434.339996337890...|     0.0|\n","|[441.420013427734...|     0.0|\n","|[445.260009765625...|     0.0|\n","|[439.609985351562...|     0.0|\n","|[445.070007324218...|     0.0|\n","|[446.119995117187...|     0.0|\n","|[440.869995117187...|     0.0|\n","|[438.609985351562...|     0.0|\n","|[431.579986572265...|     0.0|\n","|[419.410003662109...|     0.0|\n","|[410.369995117187...|     0.0|\n","+--------------------+--------+\n","only showing top 20 rows\n","\n","+-------+-------------------+\n","|summary|           WinIndex|\n","+-------+-------------------+\n","|  count|              27318|\n","|   mean|0.47488835200234275|\n","| stddev| 0.4993781471597771|\n","|    min|                0.0|\n","|    max|                1.0|\n","+-------+-------------------+\n","\n"]}]},{"cell_type":"code","source":["# Data Partioning: Split the training and testing set and verify.\n","train_data,test_data = final_data.randomSplit([0.7,0.3])"],"metadata":{"id":"_Tej2OLVH4RC","executionInfo":{"status":"ok","timestamp":1696920400823,"user_tz":-780,"elapsed":411,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":231,"outputs":[]},{"cell_type":"code","source":["# Let's import the relevant classifiers.\n","from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n","from pyspark.ml import Pipeline\n","\n","# Use defaults to make the comparison \"fair\". This simplifies the comparison process.\n","dtc = DecisionTreeClassifier(labelCol='WinIndex',featuresCol='features')\n","rfc = RandomForestClassifier(labelCol='WinIndex',featuresCol='features')\n","gbt = GBTClassifier(labelCol='WinIndex',featuresCol='features')"],"metadata":{"id":"eVuhgPyaJplO","executionInfo":{"status":"ok","timestamp":1696920403774,"user_tz":-780,"elapsed":315,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":232,"outputs":[]},{"cell_type":"code","source":["# Train the models (it's three models, so it might take some time).\n","dtc_model = dtc.fit(train_data)\n","rfc_model = rfc.fit(train_data)\n","gbt_model = gbt.fit(train_data)"],"metadata":{"id":"BRFl84zjJ3Pl","executionInfo":{"status":"ok","timestamp":1696920425111,"user_tz":-780,"elapsed":19617,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":233,"outputs":[]},{"cell_type":"code","source":["# Test the models\n","dtc_predictions = dtc_model.transform(test_data)\n","rfc_predictions = rfc_model.transform(test_data)\n","gbt_predictions = gbt_model.transform(test_data)"],"metadata":{"id":"wArhovZTJ7PE","executionInfo":{"status":"ok","timestamp":1696920446513,"user_tz":-780,"elapsed":461,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}}},"execution_count":234,"outputs":[]},{"cell_type":"code","source":["# Evaluate the results\n","# Let's start off with binary classification.\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","# Note that the label column isn't named label, it's named PrivateIndex in this case.\n","my_binary_eval = BinaryClassificationEvaluator(labelCol = 'WinIndex')\n","\n","# This is the area under the curve. This indicates that the data is highly seperable.\n","print(\"Decision Tree Accuracy\")\n","print(my_binary_eval.evaluate(dtc_predictions))\n","\n","# RFC improves accuracy but also model complexity. RFC outperforms DTC in nearly every situation.\n","print(\"Random Forest Accuracy\")\n","print(my_binary_eval.evaluate(rfc_predictions))\n","\n","# We can't repeat these exact steps for GBT. If you print the schema of all three, you may be able to notice why.\n","# Instead, let's redefine the object:\n","my_binary_gbt_eval = BinaryClassificationEvaluator(labelCol='WinIndex', rawPredictionCol='prediction')\n","print(\"GBT Accuracy\")\n","print(my_binary_gbt_eval.evaluate(gbt_predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GOza9-4mV6VS","executionInfo":{"status":"ok","timestamp":1696920694285,"user_tz":-780,"elapsed":2502,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"9168f7ab-b190-4fc5-e9da-3ce506ec8fdc"},"execution_count":237,"outputs":[{"output_type":"stream","name":"stdout","text":["Decision Tree Accuracy\n","0.4711068424108791\n","Random Forest Accuracy\n","0.5936853566265421\n","GBT Accuracy\n","0.5869874524190036\n"]}]},{"cell_type":"code","source":["# Let's import the evaluator.\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","# Select (prediction, true label) and compute test error.\n","acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"WinIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","\n","dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n","rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n","gbt_acc = acc_evaluator.evaluate(gbt_predictions)\n","\n","# Let's do something a bit more complex in terms of printing, just so it's formatted nicer.\n","print(\"Here are the results!\")\n","print('-'*40)\n","print('A single decision tree has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n","print('-'*40)\n","print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n","print('-'*40)\n","print('An ensemble using GBT has an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kV77FHeyW_Ny","executionInfo":{"status":"ok","timestamp":1696920795892,"user_tz":-780,"elapsed":2253,"user":{"displayName":"Ryan CHEUNG","userId":"06418905892169207737"}},"outputId":"e4e4b10d-39d4-4b8b-98e7-b89d521b0e3c"},"execution_count":238,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results!\n","----------------------------------------\n","A single decision tree has an accuracy of: 56.79%\n","----------------------------------------\n","A random forest ensemble has an accuracy of: 57.15%\n","----------------------------------------\n","An ensemble using GBT has an accuracy of: 59.33%\n"]}]}]}